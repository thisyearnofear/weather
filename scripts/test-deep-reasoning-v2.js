#!/usr/bin/env node
/**
 * Test Deep Reasoning with Thinking Output Handling
 * 
 * CRITICAL FINDING:
 * Qwen3-235B generates <think></think> tags during reasoning
 * We need to either:
 * 1. Strip them with strip_thinking_response parameter
 * 2. Extract reasoning for display
 * 3. Disable thinking entirely with disable_thinking parameter
 */

import OpenAI from 'openai';
import { readFileSync } from 'fs';

// Load environment variables manually
const envContent = readFileSync('.env.local', 'utf-8');
const envVars = {};
envContent.split('\n').forEach(line => {
  const match = line.match(/^([A-Z_]+)=(.+)$/);
  if (match) {
    envVars[match[1]] = match[2].trim();
  }
});

const VENICE_API_KEY = envVars.VENICE_API_KEY || process.env.VENICE_API_KEY;

const client = new OpenAI({
  apiKey: VENICE_API_KEY,
  baseURL: 'https://api.venice.ai/api/v1',
});

// Test market data
const testMarket = {
  title: 'Will Manchester City beat Liverpool in the Premier League this season?',
  eventType: 'Soccer',
  participants: ['Manchester City', 'Liverpool'],
  location: { name: 'Manchester, England' },
  currentOdds: { yes: 0.58, no: 0.42 },
  weatherData: {
    current: {
      temp_f: 48,
      condition: { text: 'Rainy' },
      precip_chance: 75,
      wind_mph: 18
    }
  }
};

console.log('ğŸ§  Testing Deep Reasoning with Thinking Output Handling\n');
console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

const systemPrompt = `You are an expert sports betting analyst specializing in weather impacts on game outcomes. Provide SPECIFIC, ACTIONABLE analysis with clear reasoning.

STRICT REQUIREMENTS:
- Tailor analysis to the given sport and participants only
- Do NOT reuse or reference any example content; generate event-specific analysis
- You MUST respond with ONLY a valid JSON object, no other text before or after
- Do NOT wrap the JSON in markdown code blocks
- Output a single JSON object with the required fields only`;

const userPrompt = `EVENT CONTEXT
- Event Title: ${testMarket.title}
- Event Type: ${testMarket.eventType}
- Participants: ${testMarket.participants.join(' vs ')}
- Venue: ${testMarket.location.name}

WEATHER
- Temperature: ${testMarket.weatherData.current.temp_f}Â°F
- Condition: ${testMarket.weatherData.current.condition.text}
- Precipitation chance: ${testMarket.weatherData.current.precip_chance}%
- Wind: ${testMarket.weatherData.current.wind_mph} mph

MARKET ODDS: YES: ${(testMarket.currentOdds.yes * 100).toFixed(1)}%, NO: ${(testMarket.currentOdds.no * 100).toFixed(1)}%

RESPONSE FORMAT - You MUST respond with ONLY this JSON structure, no other text:
{
  "weather_impact": "LOW|MEDIUM|HIGH",
  "odds_efficiency": "FAIR|OVERPRICED|UNDERPRICED|UNKNOWN",
  "confidence": "LOW|MEDIUM|HIGH",
  "analysis": "Event-specific reasoning only, no example content",
  "key_factors": ["specific, measurable factors"],
  "recommended_action": "Clear recommendation"
}`;

// Enhanced test configurations
const tests = [
  {
    name: 'ğŸŸ¢ CONTROL: Llama 3.3 70B',
    model: 'llama-3.3-70b',
    venice_parameters: {
      enable_web_search: 'auto'
    },
    temperature: 0.3,
    max_tokens: 1000
  },
  {
    name: 'ğŸ”µ REASONING V1: Qwen3-235B (Strip Thinking)',
    model: 'qwen3-235b',
    venice_parameters: {
      enable_web_search: 'auto',
      strip_thinking_response: true  // â† Key parameter to remove <think> tags
    },
    temperature: 0.3,
    max_tokens: 2000
  },
  {
    name: 'ğŸŸ£ REASONING V2: Qwen3-235B (Disable Thinking)',
    model: 'qwen3-235b',
    venice_parameters: {
      enable_web_search: 'auto',
      disable_thinking: true  // â† Alternative: disable reasoning entirely
    },
    temperature: 0.3,
    max_tokens: 1000
  }
];

// Function to extract thinking and JSON
function parseResponse(content) {
  let thinking = null;
  let jsonContent = content.trim();
  
  // Extract thinking if present
  if (jsonContent.includes('<think>')) {
    const thinkStart = jsonContent.indexOf('<think>') + 7;
    const thinkEnd = jsonContent.indexOf('</think>');
    if (thinkEnd !== -1) {
      thinking = jsonContent.substring(thinkStart, thinkEnd).trim();
      jsonContent = jsonContent.substring(thinkEnd + 8).trim();
    }
  }
  
  // Clean markdown
  if (jsonContent.startsWith('```')) {
    jsonContent = jsonContent.replace(/```json\n?|\n?```/g, '').trim();
  }
  
  // Extract JSON object
  const jsonMatch = jsonContent.match(/\{[\s\S]*\}/);
  if (jsonMatch) {
    jsonContent = jsonMatch[0];
  }
  
  return { thinking, jsonContent };
}

// Run tests
const results = [];

for (const test of tests) {
  console.log(`\n${test.name}`);
  console.log('â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');
  console.log(`Model: ${test.model}`);
  console.log(`Venice Params: ${JSON.stringify(test.venice_parameters)}`);
  
  try {
    const startTime = Date.now();
    
    const response = await client.chat.completions.create({
      model: test.model,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature: test.temperature,
      max_tokens: test.max_tokens,
      venice_parameters: test.venice_parameters
    });
    
    const endTime = Date.now();
    const responseTime = (endTime - startTime) / 1000;
    
    const rawContent = response.choices[0].message.content;
    const { thinking, jsonContent } = parseResponse(rawContent);
    
    console.log(`\nâ±ï¸  Response time: ${responseTime.toFixed(2)}s`);
    console.log(`ğŸ“ Raw response length: ${rawContent.length} characters`);
    
    if (thinking) {
      console.log(`ğŸ¤” Thinking content: ${thinking.length} characters`);
      console.log(`   Preview: "${thinking.substring(0, 100)}..."`);
    }
    
    // Try to parse JSON
    let parsed;
    let success = false;
    
    try {
      parsed = JSON.parse(jsonContent);
      success = true;
      console.log('\nâœ… JSON parsed successfully');
      
      // Display results
      console.log('\nğŸ“‹ Analysis Results:');
      console.log(`  Weather Impact: ${parsed.weather_impact}`);
      console.log(`  Odds Efficiency: ${parsed.odds_efficiency}`);
      console.log(`  Confidence: ${parsed.confidence}`);
      
      console.log('\nğŸ“ Analysis Text:');
      const analysisPreview = parsed.analysis.substring(0, 120);
      console.log(`  "${analysisPreview}..."`);
      console.log(`  (${parsed.analysis.length} characters total)`);
      
      console.log('\nğŸ”‘ Key Factors:');
      if (Array.isArray(parsed.key_factors)) {
        parsed.key_factors.forEach((factor, i) => {
          console.log(`  ${i + 1}. ${factor}`);
        });
      }
      
      console.log('\nğŸ’¡ Recommended Action:');
      console.log(`  "${parsed.recommended_action}"`);
      
      // Quality assessment
      console.log('\nğŸ“Š Quality Metrics:');
      const analysisLength = parsed.analysis.length;
      const depthScore = analysisLength > 300 ? 'Very High' : analysisLength > 150 ? 'High' : 'Medium';
      const specificFactors = parsed.key_factors.filter(f => f.length > 30).length;
      const hasLogic = parsed.analysis.includes('because') || parsed.analysis.includes('due to') || parsed.analysis.includes('impact');
      
      console.log(`  âœ“ Analysis depth: ${depthScore} (${analysisLength} chars)`);
      console.log(`  âœ“ Key factors specificity: ${specificFactors}/${parsed.key_factors.length} specific`);
      console.log(`  âœ“ Causal reasoning: ${hasLogic ? 'Yes' : 'No'}`);
      
      results.push({
        name: test.name,
        success: true,
        responseTime,
        analysisLength,
        hasThinking: !!thinking,
        depthScore,
        specificFactors,
        hasLogic
      });
      
    } catch (parseErr) {
      console.log('\nâŒ Failed to parse JSON');
      console.log(`Error: ${parseErr.message}`);
      console.log('\nJSON content (first 200 chars):');
      console.log(jsonContent.substring(0, 200));
      
      results.push({
        name: test.name,
        success: false,
        responseTime,
        error: parseErr.message
      });
    }
    
  } catch (error) {
    console.log(`\nâŒ Request failed: ${error.message}`);
    results.push({
      name: test.name,
      success: false,
      error: error.message
    });
  }
}

// Summary
console.log('\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
console.log('ğŸ“Š COMPARISON SUMMARY\n');

console.log('Success Rate:');
results.forEach(r => {
  const status = r.success ? 'âœ…' : 'âŒ';
  console.log(`  ${status} ${r.name}: ${r.success ? 'PASSED' : 'FAILED'}`);
  if (r.error) console.log(`     Error: ${r.error}`);
});

const successTests = results.filter(r => r.success);
if (successTests.length > 0) {
  console.log('\nPerformance Metrics (Successful Tests):');
  console.log('Model                                    Time     Depth        Reasoning    Thinking');
  console.log('â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');
  
  successTests.forEach(r => {
    const modelName = r.name.substring(0, 39);
    const timeStr = r.responseTime.toFixed(1) + 's' + (r.responseTime > 10 ? ' âš ï¸ ' : '     ');
    const thinkStr = r.hasThinking ? 'Yes' : 'No';
    console.log(`${modelName.padEnd(40)} ${timeStr.padEnd(8)} ${r.depthScore.padEnd(12)} ${r.specificFactors}/${r.specificFactors + (3 - r.specificFactors)}${' '.repeat(11)} ${thinkStr}`);
  });
}

console.log('\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
console.log('ğŸ’¡ FINDINGS & RECOMMENDATIONS\n');

if (successTests.length < results.length) {
  console.log('âš ï¸  CRITICAL ISSUE:');
  console.log('   Qwen3-235B with default parameters generates <think> tags');
  console.log('   that break JSON parsing. Solution: Use strip_thinking_response\n');
}

const llama = results.find(r => r.name.includes('Llama'));
const qwenStrip = results.find(r => r.name.includes('Strip'));
const qwenDisable = results.find(r => r.name.includes('Disable'));

if (llama && llama.success) {
  console.log('ğŸ“Œ LLAMA 3.3 70B (Current Setup):');
  console.log(`   Response Time: ${llama.responseTime.toFixed(2)}s`);
  console.log(`   Analysis Depth: ${llama.depthScore}`);
  console.log(`   Cost: ~$0.01/analysis`);
  console.log(`   âœ“ Reliable JSON output`);
  console.log(`   âœ— No thinking transparency`);
}

if (qwenStrip && qwenStrip.success) {
  console.log('\nğŸ“Œ QWEN3-235B (Strip Thinking):');
  console.log(`   Response Time: ${qwenStrip.responseTime.toFixed(2)}s (${(qwenStrip.responseTime / llama.responseTime).toFixed(1)}x slower)`);
  console.log(`   Analysis Depth: ${qwenStrip.depthScore}`);
  console.log(`   Cost: ~$0.03/analysis (3x more)`);
  console.log(`   âœ“ Much deeper analysis`);
  console.log(`   âœ“ Better reasoning quality`);
  console.log(`   âœ— Slower response time`);
  console.log(`   âœ— Higher cost`);
}

console.log('\n\nğŸ¯ RECOMMENDATION FOR YOUR CHARGE MODEL:\n');

console.log('OPTION A: Stick with Llama 3.3 70B (BEST FOR MVP)');
console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
console.log('  Pricing: $1 = 10 credits (1 analysis/credit)');
console.log('  Cost per analysis: ~$0.01');
console.log('  Margin: 98%+');
console.log('  âœ“ Fast (good UX)');
console.log('  âœ“ Profitable');
console.log('  âœ“ Works reliably now');
console.log('  âœ— No reasoning transparency\n');

console.log('OPTION B: Switch to Qwen3-235B (BEST QUALITY)');
console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
console.log('  Pricing: $1 = 5 credits (1 analysis = 0.2 credits)');
console.log('  Cost per analysis: ~$0.03');
console.log('  Margin: 85%+');
console.log('  âœ“ Significantly better analysis');
console.log('  âœ“ Advanced reasoning');
console.log('  âœ— Slower (4-8x)');
console.log('  âœ— Lower margins\n');

console.log('OPTION C: Hybrid Tier System (FUTURE ENHANCEMENT)');
console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
console.log('  Basic Analysis: Llama 3.3 70B = 1 credit');
console.log('  Deep Analysis: Qwen3-235B = 5 credits');
console.log('  Users choose per analysis');
console.log('  âœ“ Flexibility');
console.log('  âœ“ Caters to different needs');
console.log('  âœ“ Maximizes revenue');
console.log('  âœ— More complex implementation\n');

console.log('IMPLEMENTATION NOTE:');
console.log('If switching to Qwen3-235B, use:');
console.log('  venice_parameters: {');
console.log('    enable_web_search: "auto",');
console.log('    strip_thinking_response: true  // â† CRITICAL');
console.log('  }');
console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
